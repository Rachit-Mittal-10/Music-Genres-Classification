{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Collected Data is imblanced dataset from GTZAN**\n",
    "***In this dataset we have 10 genres ('Jazz', 'Pop', 'Classical', 'Hip -pop', Blah Blah)***\n",
    "\n",
    "***Each files contain 100 .wav files of 30 sec audio***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "# Seaborn visualization setup\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the nested folder structure\n",
    "audio_files = glob('genres_original/*/*.wav') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Oversample Minority Classes**                ## ye wala part skip kr sakte ho mei gtzan wala utha liya \n",
    "\n",
    "***Using oversample minority classes to balance the dataset to prevent bias towards majority classes during model training.***\n",
    "\n",
    "***Beneficial because balanced datasets help models learn more effectively from minority classes, leading to better generalization.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Oversample files\n",
    "# oversampled_files = []\n",
    "# for genre, count in genre_file_count.items():\n",
    "#     genre_path = os.path.join(base_path, genre)\n",
    "#     genre_files = glob(os.path.join(genre_path, '*.wav'))\n",
    "#     if count < max_count:\n",
    "#         oversampled_files.extend(genre_files)\n",
    "#         # Duplicate files to match the max_count\n",
    "#         oversampled_files.extend(resample(genre_files, replace=True, n_samples=max_count - count))\n",
    "#     else:\n",
    "#         oversampled_files.extend(genre_files)\n",
    "\n",
    "# print(f\"Total files after oversampling: {len(oversampled_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plotting the first audio file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(audio_files[855])\n",
    "print(f'y: {y[:10]}')\n",
    "print(f'shape y: {y.shape}')\n",
    "print(f'Type of y: {type(y)}')\n",
    "print(f'sr: {sr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Apply data augmentation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(y, sr):\n",
    "    # Pitch shifting\n",
    "    y_pitch_shifted = librosa.effects.pitch_shift(y, sr, n_steps=4)\n",
    "    # Time stretching\n",
    "    y_time_stretched = librosa.effects.time_stretch(y, 1.5)\n",
    "    # Adding noise\n",
    "    noise = np.random.randn(len(y))\n",
    "    y_noisy = y + 0.005 * noise\n",
    "    return [y_pitch_shifted, y_time_stretched, y_noisy]\n",
    "\n",
    "# Plot the first audio file\n",
    "# y, sr = librosa.load(audio_files[101])\n",
    "# print(f'y: {y[:10]}')\n",
    "# print(f'shape y: {y.shape}')\n",
    "# print(f'sr: {sr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plotting Raw audio** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).plot(figsize=(10, 5),lw=1, title='Raw Audio Example', color=color_pal[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trim leading/lagging silence**\n",
    "***Here we removing the first few seconds audio which can cause trouble***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
    "pd.Series(y_trimmed).plot(figsize=(10, 5), lw=1, title='Raw Audio Trimmed', color=color_pal[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Zoomed-in view of raw audio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.Series(y[30000:35000]).plot(figsize=(10, 5), lw=1, title='Raw Audio Zoomed In', color=color_pal[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Spectrogram**\n",
    "***Helps in transforming the time domain (raw audio waveform) to the frequency domain helps us understand the different frequency components present in the audio signal***\n",
    "\n",
    "\n",
    "The Short-Time Fourier Transform (STFT) is used here to analyze the signal in short overlapping time segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction\n",
    "D = librosa.stft(y)\n",
    "D_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "img = librosa.display.specshow(D_db, x_axis='time', y_axis='log', ax=ax)\n",
    "ax.set_title('Spectrogram Waveform', fontsize=20)\n",
    "fig.colorbar(img, ax=ax, format='%0.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mel Spectogram** mfcc\n",
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction\n",
    "S = librosa.feature.melspectrogram(y=y,sr=sr,n_mels=128,)\n",
    "S_db_mel = librosa.amplitude_to_db(S, ref=np.max)\n",
    "S_power_deb_mel = librosa.power_to_db(S, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plotting mel spectogram\n",
    "img = librosa.display.specshow(S_db_mel,x_axis='time',y_axis='log',ax=ax)\n",
    "ax.set_title('Mel Spectogram Waveform ', fontsize=20)\n",
    "fig.colorbar(img, ax=ax, format=f'%0.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plotting mel spectogram\n",
    "img = librosa.display.specshow(S_power_deb_mel,x_axis='time',y_axis='log',ax=ax)\n",
    "# img = librosa.display.specshow(S_power_deb_mel)\n",
    "ax.set_title('Mel Spectogram Waveform ', fontsize=20)\n",
    "fig.colorbar(img, ax=ax, format=f'%0.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc = librosa.feature.mfcc(S=S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "sns.heatmap(mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D\n",
    "\n",
    "D_db\n",
    "\n",
    "S\n",
    "\n",
    "S_db_mel\n",
    "\n",
    "S_power_db_mel\n",
    "\n",
    "mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_db_mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_power_deb_mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
